# Solutions to Assignment 1

## 1. What are Channels and Kernels (according to EVA)?

Channels are, in simplest terms, feature containers. Let us take a small example to understand this. We shall consider a corpus consisting of the letters from A-Z. To recognize these letters we will have 26 channels where each channel will be designed to capture the feature of a particular letter. Try to imagine 26 buckets each labelled with the name of the letter is supposed to contain. If we have say Bucket 'F' then all types of F's—different colours, sizes, shapes etc.—can be stored in this bucket. The same holds for each of the 26 buckets that we have with us. This is precisely what a channel is. It is a container for features. Another example can be an image then the channels will be primary colours in that image. If there is an RGB image then the channels would be "R", "G" and "B".  

Kernels are, in simplest terms, feature extractors. Now that we know channels are, let us think of Kernels as feature extractors, that is, they can extract the letters from the buckets. Now, if the kernel picks up the letter 'F' ( assume it's a feature ) from our set of buckets then it becomes the feature extractor of the feature 'F'/letter 'F' from the channel bucket labelled F. In case of image processing, Kernels is a matrix of weights, which is slid over the input(which consist of channels) and the dot-product of the kernel matrix and the input will then highlight the feature. 

## 2. Why should we (nearly) always use 3x3 kernels?

a) As the kernel is slid over the input image—made of pixels, containing information it makes use of these pixels—the dot product of the kernel matrix and the input—to extract certain features ( say edges ), it is important to note that in case of an even-sized kernel ( say 2 * 2 ) we cannot make use of the symmetric structure of the neighbouring pixels whereas in case of an odd-sized kernel ( say 3 * 3) we can make use of both the neighbours to the left of the final output pixel as well as the right. Note: the final output pixel of the next layer is generated by the convolution on the previous layer pixels. 

b) The computational cost in the case of 3 * 3 is better than other odd-sized kernels. Say for example we want a 1 * 1 output pixel from a 5 * 5 ( that is the receptive field of the final output pixel must be 5 * 5 ) then we could use a 5 *  5 which would have a total of 25 parameters or we could go for two 3 * 3 kernel matrices which would have a total of 18 parameters. Since the number of parameters is less in case 2 and subsequently the computational cost we prefer 3 * 3 kernels. A doubt might surface here. Why not a 1 * 1 kernel then? In the case of features extracted in this case, they would be fine-grained and local, with no consideration for the neighbouring pixels.

c) Originally, NVIDIA GPU's were optimized for 3 * 3 kernels. When 3 * 3 kernels were widely, for this reason, NVIDIA started producing more GPU's optimized for 3 * 3 kernels, and so on. 

## 3. How many times do we need to perform 3x3 convolutions operations to reach close to 1x1 from 199x199 (type each layer output like 199x199 > 197x197...)

Let us devise a formula for this:

Here we perform 3 * 3 convolution operation at each stage. 

7 * 7 --> 5 * 5 --> 3 * 3 --> 1* 1  === Takes 3 computations. 
5 * 5 --> 3 * 3 --> 1* 1  === Takes 2 computations. 
3 * 3 --> 1* 1  === Takes 1 computation.

Thus, we see a pattern here. The number of computations is (input-size - kernel size)/2 + 1.

In case of the question: input size = 199; Kernel size = 3
Therefore, no of computations = (199-3)/2 + 1 = 99. 

Let us see this with the example of a code. 


```python
convolution_layers = ['{}x{} >'.format(size, size)  for size in  range(199,  0,  -2)]
# print(*convolution_layers)
string = ''.join(convolution_layers)
print(string[:-1])  
print('\nNumber of 3x3 convolutions = {}'.format(len(range(199,  0,  -2))-1))
```

    199x199 >197x197 >195x195 >193x193 >191x191 >189x189 >187x187 >185x185 >183x183 >181x181 >179x179 >177x177 >175x175 >173x173 >171x171 >169x169 >167x167 >165x165 >163x163 >161x161 >159x159 >157x157 >155x155 >153x153 >151x151 >149x149 >147x147 >145x145 >143x143 >141x141 >139x139 >137x137 >135x135 >133x133 >131x131 >129x129 >127x127 >125x125 >123x123 >121x121 >119x119 >117x117 >115x115 >113x113 >111x111 >109x109 >107x107 >105x105 >103x103 >101x101 >99x99 >97x97 >95x95 >93x93 >91x91 >89x89 >87x87 >85x85 >83x83 >81x81 >79x79 >77x77 >75x75 >73x73 >71x71 >69x69 >67x67 >65x65 >63x63 >61x61 >59x59 >57x57 >55x55 >53x53 >51x51 >49x49 >47x47 >45x45 >43x43 >41x41 >39x39 >37x37 >35x35 >33x33 >31x31 >29x29 >27x27 >25x25 >23x23 >21x21 >19x19 >17x17 >15x15 >13x13 >11x11 >9x9 >7x7 >5x5 >3x3 >1x1 
    
    Number of 3x3 convolutions = 99
    

## 4. How are kernels initialized?

Let us go over the a few cases to understand this. We can understand this  

a) The kernel is initialized with equal weights. 
b) The kernel is initialized with large weights. 
c) The kernel is initialized with small weights.
d) The kernel is initialized with zero weights. 


a) In this case, the equality is carried through to the gradients associated with these weights.This keeps the weights equal throughout the training thus losing the very purpose of a building a network whose task is to find optimal weights. 
b) This can lead to exploding gradient problem. This results in very large updates to neural network model weights during training.
c) This can lead to vanishing gradient problem. result in very small updates (minuscule) to neural network model weights during training preventing the weight from changing its value.
d) In this case, the equations of the learning algorithm would fail to make any changes to the network weights, since the weights are zero, and the model will be stuck.

Therefore, we initialize weights randomly, using weights from a Gaussian Distribution, or weights from a distribution with zero mean and a specific variance. The first case is Xavier initialization. The second is He initialization etc. These are some of the ways the kernel values are initialized. 


## 5. What happens during the training of a DNN?

The deep neural network consists of: 

a) Convolution layers. 
b) Pooling layers. 
c) Fully connected layers. 


Initially, the kernel is initialized with weights/normally distributed random numbers. This kernel slides over the input image and learns certain features. This happens by the summation of the dot-product of the kernel matrix with the input image. During initial layers, the CNN learns about edges and gradients, textures and patterns in middle layers and later layers CNN learns about parts of objects and complete objects. This happens as it passes through the several layers mentioned above. Then the output image is compared with the original image to determine the "loss" ( how similar are the two images ? ). Then through the technique of backpropagation, the weights are re-adjusted to minimize the loss. This happens over several iterations of "epochs" after which the model is said to be trained. 

Analogy: Consider a group of bots assigned to a teacher-bot. They're tasked with recognizing a cat and a dog from a set of images. Now, the set of images and the answer keys are handed over to the teacher-bot who is as naive as the student-bots. The teacher-bot, having the answer keys, starts testing the student-bots after a small "teaching" session. They're very bad at what they do, all of them. However, with each iteration, the best of the student-bots are retained (those who scored better than their peers) and the rest are expelled. Again, the student-bots go through the gruelling test assigned by the teacher-bots ( who has the answer keys ). This cycle of teaching, test, expel, repeat is continued over and over again until the student-bots have learnt to say a cat from a dog with a certain confidence. Remember, they did not know anything at first and with enough iterations, learning and "expelling" they were able to do the job of recognizing a cat from a dog. This is a very high-level overview of how the machine learns.
