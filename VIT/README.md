Transformers, which are based on self-attention, have become the default model for natural language processing (NLP). In general, large texts are pre-trained on, and then fine-tuned on, a smaller task-specific dataset (Devalin et al., 2019). This is due to Transformers' speed and scalability. Convolutional architectures, however, remain popular in computer vision. Because Natural Language Processing has been so successful, researchers have been using CNN-like architectures and self-attention simultaneously. 
